import nn as nn
from tensor import Tensor,autograd_function
import optimizer as optim
import dataloader as dataloader
import functional as F
import tensor as TS
class LinearRegression(nn.Module):
    """
    Linear Regressoin Module, the input features and output 
    features are defaults both 1
    """
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(3,1)

    @buildingblock("log-Regressoin")    
    def forward(self,x):
        out = self.linear(x)
        out = F.sigmoid(out)
        return out

x = MultiArray([3,3], sfix)
y = MultiArray([3,1], sfix)
for i in range(x.total_size()):
    x.assign_vector(sfix(i), i)

for i in range(y.total_size()):
    y.assign_vector(sfix(i), i)

dataload = dataloader.DataLoader(x, y, batch_size = 2, shuffle=False)

model = LinearRegression()

optimizer = optim.Adam(model.parameters(), lr = 0.01)
params = list(model.parameters())
params[0].assign_all(0.1)
for i in params:
    i.assign_all(0)
mse = nn.MSELoss()
model.train(mse, dataload)

@for_range(10)
def _(i):
    input, label = dataload.get_data(0)
    output = model(input)
    loss = mse(output, label)
    optimizer.zero_grad()
    loss.backward()
    params[0].print_reveal_nested()
    loss.print_reveal_nested()
    optimizer.step()